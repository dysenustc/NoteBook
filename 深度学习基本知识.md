CNN(Convolutional Neural Networks/卷积神经网络)、RNN(Recurrent Neural Networks/循环神经网络)、DNN(深度神经网络)
感知机（perceptron）
多层感知机（multilayer perceptron） 多层隐含层
卷积神经网络：
通过“卷积核”作为中介，同一个卷积核在所有图像内是共享的，图像通过卷积操作后仍然保留原先的位置关系。
Pooling Layer 分池层

循环神经网络：
全连接的DNN还存在着另一个问题————无法对时间序列上的变化进行建模。然而，样本出现的时间顺序对于自然语言处理、
语音识别、手写体识别等应用非常重要。
在RNN中，神经元的输出可以在下一个时间戳直接左右到自身。
RNN可以看成一个在时间上传递的神经网络，它的深度是时间的长度。

LSTM（Long Short Term Memory/长短时记忆单元）
通过门的开关实现时间上记忆功能，并防止梯度消失。

双向RNN、双向LSTM，同时利用历史和未来的信息。



深度学习：UFLDL教程（Andrew Ng）:
http://ufldl.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B

Pycon 2016 tensorflow 研讨会总结 — tensorflow 手把手入门:
http://nooverfit.com/wp/pycon-2016-tensorflow-%E7%A0%94%E8%AE%A8%E4%BC%9A%E6%80%BB%E7%BB%93-tensorflow-%E6%89%8B%E6%8A%8A%E6%89%8B%E5%85%A5%E9%97%A8/
http://nooverfit.com/wp/pycon-2016-tensorflow-%E7%A0%94%E8%AE%A8%E4%BC%9A%E6%80%BB%E7%BB%93-tensorflow-%E6%89%8B%E6%8A%8A%E6%89%8B%E5%85%A5%E9%97%A8-%E7%AC%AC%E4%BA%8C%E8%AE%B2-word2vec/
http://nooverfit.com/wp/pycon-2016-tensorflow-%E7%A0%94%E8%AE%A8%E4%BC%9A%E6%80%BB%E7%BB%93-tensorflow-%E6%89%8B%E6%8A%8A%E6%89%8B%E5%85%A5%E9%97%A8-%E7%94%A8%E4%BA%BA%E8%AF%9D%E8%A7%A3%E9%87%8Acnn-%E7%AC%AC%E4%B8%89/

